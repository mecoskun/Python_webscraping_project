{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RDS@GSU - Web Scraping Continued (Workshop # 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Copyright + References"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The content in this notebook was developed by Jeremy Walker.\n",
    "# All sample code and notes are provided under a Creative Commons\n",
    "# ShareAlike license.\n",
    "\n",
    "# Official Copyright Rules / Restrictions / Priveleges\n",
    "# Attribution-ShareAlike 4.0 International (CC BY-SA 4.0)\n",
    "# https://creativecommons.org/licenses/by-sa/4.0/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1 - Refresher and Recap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify a webpage\n",
    "state_dept = 'https://www.state.gov/press-releases/'\n",
    "state_dept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get the webpage\n",
    "webpage = requests.get(url = state_dept)\n",
    "webpage.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the webpage\n",
    "html_file = open(file = \"web_documents/press_release_directory.html\", mode = \"w\", encoding = \"utf-8\")\n",
    "html_file.write(webpage.text)\n",
    "html_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create the soup\n",
    "soup = BeautifulSoup(markup = webpage.text)\n",
    "soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create a list of results by finding all links \"a\" within the \"collection-results\" element.\n",
    "results = soup.find(class_=\"collection-results\").find_all(name = \"a\")\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an object (first_result) from the first item in the \"results\" list\n",
    "first_result = results[0]\n",
    "first_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get the webpage for the first_result\n",
    "first_webpage = requests.get(url=first_result['href'])\n",
    "first_webpage.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the new webpage to a new file\n",
    "html_file = open(file = \"web_documents/first_webpage.html\", mode = \"w\", encoding = \"utf-8\")\n",
    "html_file.write(first_webpage.text)\n",
    "html_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Altogether..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Specify a webpage\n",
    "state_dept = 'https://www.state.gov/press-releases/'\n",
    "\n",
    "# Get the webpage\n",
    "webpage = requests.get(url = state_dept)\n",
    "\n",
    "# Save the webpage\n",
    "html_file = open(file = \"web_documents/press_release_directory.html\", mode = \"w\", encoding = \"utf-8\")\n",
    "html_file.write(webpage.text)\n",
    "html_file.close()\n",
    "\n",
    "# Create the soup\n",
    "soup = BeautifulSoup(markup = webpage.text)\n",
    "\n",
    "# Create a list of results by finding all links \"a\" within the \"collection-results\" element.\n",
    "results = soup.find(class_=\"collection-results\").find_all(name = \"a\")\n",
    "\n",
    "# Create an object (first_result) from the first item in the \"results\" list\n",
    "first_result = results[0]\n",
    "\n",
    "# Get the webpage for the first_result\n",
    "first_webpage = requests.get(url=first_result['href'])\n",
    "\n",
    "# Save the new webpage to a new file\n",
    "html_file = open(file = \"web_documents/first_webpage.html\", mode = \"w\", encoding = \"utf-8\")\n",
    "html_file.write(first_webpage.text)\n",
    "html_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2 - Iterating through content "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What we're building towards...\n",
    "The block of code below represents the entirety of what Part 2 and Part 3 are building towards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import requests\n",
    "# from bs4 import BeautifulSoup\n",
    "# def scraper( starting_url , count):\n",
    "#     website = starting_url\n",
    "#     page_counter = count\n",
    "#     webpage = requests.get(url = website)\n",
    "#     soup = BeautifulSoup(markup = webpage.text)\n",
    "#     results = soup.find(class_=\"collection-results\").find_all(name = \"a\")\n",
    "#     for page in results:\n",
    "#         page_url = page['href']\n",
    "#         webpage = requests.get(page_url)\n",
    "#         filename = \"web_documents/{}.html\".format(page_counter)\n",
    "#         html_file= open(file = filename, mode = \"w\", encoding=\"utf-8\")\n",
    "#         html_file.write(webpage.text)\n",
    "#         html_file.close()\n",
    "#         page_counter = page_counter + 1\n",
    "#     if soup.find(class_=\"next page-numbers\"):\n",
    "#         next_page = soup.find(class_=\"next page-numbers\")\n",
    "#         next_url = next_page['href']\n",
    "#         if page_counter < 30:\n",
    "#             scraper(starting_url = next_url, count = page_counter)\n",
    "# scraper(starting_url=\"https://www.state.gov/press-releases/\", count=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Specify a webpage\n",
    "state_dept = 'https://www.state.gov/press-releases/'\n",
    "\n",
    "# Get the webpage\n",
    "webpage = requests.get(url = state_dept)\n",
    "\n",
    "# Save the webpage\n",
    "html_file = open(file = \"web_documents/press_release_directory.html\", mode = \"w\", encoding = \"utf-8\")\n",
    "html_file.write(webpage.text)\n",
    "html_file.close()\n",
    "\n",
    "# Create the soup\n",
    "soup = BeautifulSoup(markup = webpage.text)\n",
    "\n",
    "# Create a list of results by finding all links \"a\" within the \"collection-results\" element.\n",
    "results = soup.find(class_=\"collection-results\").find_all(name = \"a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The results object is a list of individual \"a\" tag objects.\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In order to collect each individual webpage in the results list,\n",
    "# we will use a for-loop to iterate through each item in the list.\n",
    "# For-loops can be a bit tricky to understand if you are new to \n",
    "# programming and scripting.  However, there are many excellent\n",
    "# guides and tutorials on the web for this:\n",
    "\n",
    "# For Loops Tutorials : https://www.w3schools.com/python/python_for_loops.asp\n",
    "\n",
    "# The following cells show a variety of examples of the basic syntax\n",
    "# of for-loops.\n",
    "\n",
    "# This example can be read as \"For every item ('i') in a list \n",
    "# ('list_of_items'), display ('print()') the item ('i').\n",
    "\n",
    "list_of_items = [\"a\",\"b\",\"c\",\"d\"]\n",
    "\n",
    "for i in list_of_items:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# While 'i' is a common placeholder for items when iterating\n",
    "# through a list, it is arbitrary.  Any other placeholder will\n",
    "# function exactly the same.\n",
    "\n",
    "list_of_items = [\"a\",\"b\",\"c\",\"d\"]\n",
    "\n",
    "for x in list_of_items:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for banana in list_of_items:\n",
    "    print(banana)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# While for-loops are usually used to perform\n",
    "# some function or operation on a different item,\n",
    "# that is not strictly required.  Python will do\n",
    "# an arbitrary task for each iteration.\n",
    "\n",
    "for banana in list_of_items:\n",
    "    print(\"oranges\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returning to the results object, the following\n",
    "# code iterates through each result and displays\n",
    "# the tag.\n",
    "\n",
    "for page in results:\n",
    "    print(\"PAGE ITEM\")\n",
    "    print(page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Within the for-loop, you can define a new object\n",
    "# or perform a function / operation and display the results.\n",
    "\n",
    "# In the following example, the \"page\" placeholder represents\n",
    "# the individual tag object from the results list.  Consequently,\n",
    "# the \"page\" placeholder has the same characteristics as the\n",
    "# item it represents.  In this case, that means we can get the\n",
    "# \"href\" attribute.\n",
    "\n",
    "for page in results:\n",
    "    page_url = page['href']\n",
    "    print(page_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For loops can also modify existing objects. For example,\n",
    "# we can create an object that simply increases by 1 for\n",
    "# every iteration in the for loop.\n",
    "\n",
    "page_counter = 0\n",
    "\n",
    "for page in results:    \n",
    "    \n",
    "    print(page_counter) \n",
    "    \n",
    "    page_counter = page_counter + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expand the for-loop to include both the counter and\n",
    "# the hyperlink (\"href\").\n",
    "\n",
    "page_counter = 0\n",
    "\n",
    "for page in results:\n",
    "    page_url = page['href']\n",
    "    \n",
    "    print(page_counter)\n",
    "    print(page_url)\n",
    "    \n",
    "    page_counter = page_counter + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Altogether, we can create a for-loop\n",
    "# that iterates through the results list, collects each\n",
    "# individual webpage and saves them using an enumerated\n",
    "# filename.\n",
    "\n",
    "page_counter = 0\n",
    "\n",
    "for page in results:\n",
    "    \n",
    "    # Specify a webpage\n",
    "    page_url = page['href']\n",
    "    \n",
    "    # Get the webpage\n",
    "    webpage = requests.get(page_url)\n",
    "    \n",
    "    # Create filename\n",
    "    # This is a little complicated, but in essence, the {}\n",
    "    # inside a string allows us to dynamically change the \n",
    "    # text of a string.  In this case, the .format(...)\n",
    "    # specifies that the filename will be the name of the\n",
    "    # html document once it is saved to the computer.\n",
    "    filename = \"web_documents/{}.html\".format(page_counter)\n",
    "    \n",
    "    # Save the webpage\n",
    "    html_file= open(file = filename, mode = \"w\", encoding=\"utf-8\")\n",
    "    html_file.write(webpage.text)\n",
    "    html_file.close()\n",
    "    \n",
    "    # Sanity-check: print out the URL and current page_counter\n",
    "    print(page_counter)\n",
    "    print(page_url)\n",
    "    \n",
    "    # Update the page_counter\n",
    "    page_counter = page_counter + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2 Recap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Specify a webpage\n",
    "state_dept = 'https://www.state.gov/press-releases/'\n",
    "\n",
    "# Get the webpage\n",
    "webpage = requests.get(url = state_dept)\n",
    "\n",
    "# Save the webpage\n",
    "html_file = open(file = \"web_documents/press_release_directory.html\", mode = \"w\", encoding = \"utf-8\")\n",
    "html_file.write(webpage.text)\n",
    "html_file.close()\n",
    "\n",
    "# Create the soup\n",
    "soup = BeautifulSoup(markup = webpage.text)\n",
    "\n",
    "# Create a list of results by finding all links \"a\" within the \"collection-results\" element.\n",
    "results = soup.find(class_=\"collection-results\").find_all(name = \"a\")\n",
    "\n",
    "# Initiate counter\n",
    "page_counter = 0\n",
    "\n",
    "# For Loop\n",
    "for page in results:\n",
    "    \n",
    "    # Specify a webpage\n",
    "    page_url = page['href']\n",
    "    \n",
    "    # Get the webpage\n",
    "    webpage = requests.get(page_url)\n",
    "    \n",
    "    # Create filename\n",
    "    filename = \"web_documents/{}.html\".format(page_counter)\n",
    "    \n",
    "    # Save the webpage\n",
    "    html_file= open(file = filename, mode = \"w\", encoding=\"utf-8\")\n",
    "    html_file.write(webpage.text)\n",
    "    html_file.close()\n",
    "    \n",
    "    # Update the page_counter\n",
    "    page_counter = page_counter + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3 - Creating a Function to Wrap Everything Together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In order to to simplify how the code above is\n",
    "# displayed and used, we will encapsulate the\n",
    "# script within a \"function\".  There is a lot\n",
    "# to learn regarding exactly how Functions work,\n",
    "# but that is beyond the scope of this workshop.\n",
    "# For this section, we'll focus on using simple\n",
    "# and straigthforward mechanics of Functions.\n",
    "\n",
    "# Functions Tutorials - https://www.w3schools.com/python/python_functions.asp\n",
    "\n",
    "# For example, the following function (\"FunctionName\") \n",
    "# takes two parameters.  In truth, you can define a\n",
    "# function to take any number of parameters or none\n",
    "# at all.  The function then does whatever operations\n",
    "# you instruct.\n",
    "\n",
    "def FunctionName(parameter1 , parameter2):\n",
    "    # Do something, anything within the function...\n",
    "    xyz = parameter1 + 500\n",
    "    abc = xyz + parameter2\n",
    "    print(abc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Once the function has been defined, we can\n",
    "# call or use that function just like any of\n",
    "# the other functions we've used.\n",
    "\n",
    "FunctionName(parameter1=500, parameter2=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FunctionName(parameter1=141, parameter2=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building from there, we can create a function\n",
    "# that will encapsulate everything we've done so \n",
    "# far with web-scraping.\n",
    "\n",
    "def scraper (starting_url , count):\n",
    "    # insert appropriate lines of code here\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Putting it all together, the function below will\n",
    "# use the starting_url we provide and the count we \n",
    "# provide to go to a website, collect all of the URLs\n",
    "# for individual Press Release documents, then download\n",
    "# and save individual documents.\n",
    "\n",
    "# Import modules\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def scraper( starting_url , count):\n",
    "\n",
    "    # Specify a webpage\n",
    "    website = starting_url\n",
    "\n",
    "    # Initiate counter\n",
    "    page_counter = count\n",
    "\n",
    "    # Get the webpage\n",
    "    webpage = requests.get(url = website)\n",
    "\n",
    "    # Create the soup\n",
    "    soup = BeautifulSoup(markup = webpage.text)\n",
    "\n",
    "    # Create a list of results by finding all links \"a\" within the \"collection-results\" element.\n",
    "    results = soup.find(class_=\"collection-results\").find_all(name = \"a\")\n",
    "\n",
    "    # For Loop\n",
    "    for page in results:\n",
    "\n",
    "        # Specify a webpage\n",
    "        page_url = page['href']\n",
    "\n",
    "        # Get the webpage\n",
    "        webpage = requests.get(page_url)\n",
    "\n",
    "        # Create filename\n",
    "        filename = \"web_documents/{}.html\".format(page_counter)\n",
    "\n",
    "        # Save the webpage\n",
    "        html_file= open(file = filename, mode = \"w\", encoding=\"utf-8\")\n",
    "        html_file.write(webpage.text)\n",
    "        html_file.close()\n",
    "\n",
    "        # Update the page_counter\n",
    "        page_counter = page_counter + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lastly, call or execute the script!\n",
    "scraper(starting_url=\"https://www.state.gov/press-releases/\", count=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# But wait, there's more!!\n",
    "\n",
    "# In addition to writing a function to excecute a single\n",
    "# set of commands, we can also set up the function in such\n",
    "# a way that it repeats itself multiple times.  This is\n",
    "# useful because now we can expand the function in a way\n",
    "# that allows us to go to the \"next page\" on the Press\n",
    "# Releases webpage and collect more and more documents.\n",
    "\n",
    "\n",
    "# The following comment is an oversimplified example of\n",
    "# a function executing some code and then calling itself\n",
    "# again to repeat the same code.\n",
    "\n",
    "# def scraper (parameters...):\n",
    "#     code code code\n",
    "#     code code code\n",
    "#     scraper(parameters...)\n",
    "\n",
    "# The following function has a fully fleshed out example \n",
    "# of this addition.\n",
    "\n",
    "\n",
    "# Import modules\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def scraper( starting_url , count):\n",
    "    \n",
    "    # Specify a webpage\n",
    "    website = starting_url\n",
    "\n",
    "    # Initiate counter\n",
    "    page_counter = count\n",
    "\n",
    "    # Get the webpage\n",
    "    webpage = requests.get(url = website)\n",
    "\n",
    "    # Create the soup\n",
    "    soup = BeautifulSoup(markup = webpage.text)\n",
    "\n",
    "    # Create a list of results by finding all links \"a\" within the \"collection-results\" element.\n",
    "    results = soup.find(class_=\"collection-results\").find_all(name = \"a\")\n",
    "\n",
    "    # For Loop\n",
    "    for page in results:\n",
    "\n",
    "        # Specify a webpage\n",
    "        page_url = page['href']\n",
    "\n",
    "        # Get the webpage\n",
    "        webpage = requests.get(page_url)\n",
    "\n",
    "        # Create filename\n",
    "        filename = \"web_documents/{}.html\".format(page_counter)\n",
    "\n",
    "        # Save the webpage\n",
    "        html_file= open(file = filename, mode = \"w\", encoding=\"utf-8\")\n",
    "        html_file.write(webpage.text)\n",
    "        html_file.close()\n",
    "\n",
    "        # Update the page_counter\n",
    "        page_counter = page_counter + 1\n",
    "    \n",
    "\n",
    "#     The following block of code adds recursion to the function.\n",
    "#     First -  It uses the soup to see if there is a \"next page-numbers\" tag (i.e. \"next button\")\n",
    "#     Second - If so, it extracts the URL for the next page of Press Releases results.\n",
    "#     Third -  It checks the current value of page_counter.  If the value is less than 30,\n",
    "#              it calls the \"scraper()\" function again and passes the next_url and page_counter\n",
    "#              parameters and the WHOLE scraper function repeats itself.\n",
    "#     Fourth - If page_counter is equal to or greater than 30, the scraper() function stops.\n",
    "\n",
    "    if soup.find(class_=\"next page-numbers\"):\n",
    "        next_page = soup.find(class_=\"next page-numbers\")\n",
    "        next_url = next_page['href']\n",
    "        \n",
    "        if page_counter < 30:\n",
    "            scraper(starting_url = next_url, count = page_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same code as above, but with fewer comments.\n",
    "\n",
    "# Import modules\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def scraper( starting_url , count):\n",
    "    \n",
    "    # Specify a webpage\n",
    "    website = starting_url\n",
    "\n",
    "    # Initiate counter\n",
    "    page_counter = count\n",
    "\n",
    "    # Get the webpage\n",
    "    webpage = requests.get(url = website)\n",
    "\n",
    "    # Create the soup\n",
    "    soup = BeautifulSoup(markup = webpage.text)\n",
    "\n",
    "    # Create a list of results by finding all links \"a\" within the \"collection-results\" element.\n",
    "    results = soup.find(class_=\"collection-results\").find_all(name = \"a\")\n",
    "\n",
    "    # For Loop\n",
    "    for page in results:\n",
    "\n",
    "        # Specify a webpage\n",
    "        page_url = page['href']\n",
    "\n",
    "        # Get the webpage\n",
    "        webpage = requests.get(page_url)\n",
    "\n",
    "        # Create filename\n",
    "        filename = \"web_documents/{}.html\".format(page_counter)\n",
    "\n",
    "        # Save the webpage\n",
    "        html_file= open(file = filename, mode = \"w\", encoding=\"utf-8\")\n",
    "        html_file.write(webpage.text)\n",
    "        html_file.close()\n",
    "\n",
    "        # Update the page_counter\n",
    "        page_counter = page_counter + 1\n",
    "    \n",
    "    # Advance to the next page and repeat the process\n",
    "    if soup.find(class_=\"next page-numbers\"):\n",
    "        next_page = soup.find(class_=\"next page-numbers\")\n",
    "        next_url = next_page['href']\n",
    "        \n",
    "        if page_counter < 30:\n",
    "            scraper(starting_url = next_url, count = page_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the final function one last time.\n",
    "scraper(starting_url=\"https://www.state.gov/press-releases/\", count=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3 - Recap\n",
    "The following block of code building upon Workshop-1 and everything covered so far in this workshop's code.  Putting it all together, the following dense block of code should (at the time of writing) systematically collect the 30 most recent webpages from the US State Department's Press Releases webpage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import requests\n",
    "# from bs4 import BeautifulSoup\n",
    "# def scraper( starting_url , count):\n",
    "#     website = starting_url\n",
    "#     page_counter = count\n",
    "#     webpage = requests.get(url = website)\n",
    "#     soup = BeautifulSoup(markup = webpage.text)\n",
    "#     results = soup.find(class_=\"collection-results\").find_all(name = \"a\")\n",
    "#     for page in results:\n",
    "#         page_url = page['href']\n",
    "#         webpage = requests.get(page_url)\n",
    "#         filename = \"web_documents/{}.html\".format(page_counter)\n",
    "#         html_file= open(file = filename, mode = \"w\", encoding=\"utf-8\")\n",
    "#         html_file.write(webpage.text)\n",
    "#         html_file.close()\n",
    "#         page_counter = page_counter + 1\n",
    "#     if soup.find(class_=\"next page-numbers\"):\n",
    "#         next_page = soup.find(class_=\"next page-numbers\")\n",
    "#         next_url = next_page['href']\n",
    "#         if page_counter < 30:\n",
    "#             scraper(starting_url = next_url, count = page_counter)\n",
    "# scraper(starting_url=\"https://www.state.gov/press-releases/\", count=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4 - URL Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When collecting data using web-scraping, another key feature of website to pay attention to is how URLs are used to represent information.  In some cases, URLs are static representations of webpages.  However, increasingly on many websites, URLs contain various parameters that are dynamic.  Usually indicated by a \"?\", these parameters will often contain information like the keywords you used in a search, metadata about where you are being referred to or from, etc...Some examples may help...\n",
    "\n",
    "When using the search-box on the NYTimes website, you are directed to the following URL:\n",
    "\n",
    "https://www.nytimes.com/search?query=covid\n",
    "\n",
    "Following the word \"...search\" in the URL is one parameter-value pair: \"query=covid\".  This is in effect, dynamically telling the NYTimes web server that you want the search results page for the keyword \"covid\".\n",
    "\n",
    "On the search page itself, if you change some of the search filters, you will notice that you are directed to a new and evolved URL with even more parameters:\n",
    "\n",
    "https://www.nytimes.com/search?dropmab=true&endDate=20200923&query=covid&sort=newest&startDate=20200916&types=article\n",
    "\n",
    "Some of the new URL Parameters are probably easy to interpret.  Others, like \"dropmab\" might require a little exploration and experimentation to figure out what they mean.\n",
    "- dropmab = true\n",
    "- endDate = 20200923\n",
    "- query = covid\n",
    "- sort = newest\n",
    "- startDate = 20200916\n",
    "- types = article\n",
    "\n",
    "Ultimately, being able to inspect and understand what these parameters are may give you another method for collecting information from websites...LET'S PRACTICE!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start by visiting : https://www.nytimes.com/search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Specify a webpage\n",
    "nytimes = 'https://www.nytimes.com/search'\n",
    "\n",
    "# Get the webpage\n",
    "webpage = requests.get(url = nytimes)\n",
    "\n",
    "# Create the soup\n",
    "soup = BeautifulSoup(markup = webpage.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, identify an appropriate method (e.g. find_all(...) ) for identifying data on the page.\n",
    "# In this case, I will find the HTML \"class\" that identifies the article-title for each search-result.\n",
    "soup.find_all(class_=\"css-2fgx4k\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use the NYTimes search to start a simple keyword search and inspect the URL\n",
    "#### https://www.nytimes.com/search?query=covid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the webpage, but this time include a params=... argument in the requests.get(...) function.\n",
    "# The params=... argument requires a \"dict\" or \"dictionary\" form of information.\n",
    "\n",
    "# Dict example:  { key : value }\n",
    "# Dict example:  { \"name\" : \"jeremy\" }\n",
    "\n",
    "# Dict example:  { key1 : value1 , key2 : value2 }\n",
    "# Dict example:  { \"first_name\" : \"jeremy\" , \"last_name\" : \"walker\" }\n",
    "\n",
    "# Specify a webpage\n",
    "nytimes = 'https://www.nytimes.com/search'\n",
    "\n",
    "# Get the webpage, with params\n",
    "webpage = requests.get(\n",
    "    url = nytimes,\n",
    "    params = { \"query\" : \"covid\" } \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the URL\n",
    "webpage.url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the soup\n",
    "soup = BeautifulSoup(markup = webpage.text)\n",
    "\n",
    "# Find and display the targetted results...\n",
    "soup.find_all(class_=\"css-2fgx4k\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now, using the NYTimes search page, change the \"date\" and \"type\" options on the search-page\n",
    "#### https://www.nytimes.com/search?dropmab=true&endDate=20200910&query=covid&sort=best&startDate=20200901&types=article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify a webpage\n",
    "nytimes = 'https://www.nytimes.com/search'\n",
    "\n",
    "\n",
    "# Get the webpage, with params\n",
    "webpage = requests.get(\n",
    "    url = nytimes,\n",
    "    params = { \n",
    "        \"dropmab\" : \"true\" ,\n",
    "        \"endDate\" : \"20200910\" ,\n",
    "        \"query\" : \"covid\" ,\n",
    "        \"sort\" : \"best\" ,\n",
    "        \"startDate\" : \"20200901\" ,\n",
    "        \"types\" : \"article\" ,\n",
    "    } \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the URL\n",
    "webpage.url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the soup\n",
    "soup = BeautifulSoup(markup = webpage.text)\n",
    "\n",
    "# Find and display the targetted results...\n",
    "soup.find_all(class_=\"css-2fgx4k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EVERY WEBSITE IS DIFFERENT\n",
    "# Some websites are flexible with required and optional parameters, formatting, etc...\n",
    "# The example below is based on the prior code, but with some URL parameters missing\n",
    "\n",
    "# Specify a webpage\n",
    "nytimes = 'https://www.nytimes.com/search'\n",
    "\n",
    "\n",
    "\n",
    "# Get the webpage, with params (some commented out)\n",
    "webpage = requests.get(\n",
    "    url = nytimes,\n",
    "    params = { \n",
    "#         \"dropmab\" : \"true\" ,\n",
    "        \"endDate\" : \"20200910\" ,\n",
    "        \"query\" : \"covid\" ,\n",
    "#         \"sort\" : \"best\" ,\n",
    "        \"startDate\" : \"20200901\" ,\n",
    "#         \"types\" : \"article\" ,\n",
    "    } \n",
    ")\n",
    "\n",
    "# Inspect the URL\n",
    "print(webpage.url)\n",
    "print()\n",
    "\n",
    "# Create the soup\n",
    "soup = BeautifulSoup(markup = webpage.text)\n",
    "\n",
    "# Find and display the targetted results...\n",
    "soup.find_all(class_=\"css-2fgx4k\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Iterating through content (again!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define list of keyword search terms\n",
    "keywords_list = [\"covid\",\"coronavirus\",\"pandemic\",\"CDC\"]\n",
    "\n",
    "for keyword in keywords_list:\n",
    "    print(keyword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Specify a webpage\n",
    "nytimes = 'https://www.nytimes.com/search'\n",
    "\n",
    "# Define list of keywords\n",
    "keywords_list = [\"covid\",\"coronavirus\",\"pandemic\",\"CDC\"]\n",
    "\n",
    "# Create empty list of results to add items to\n",
    "results = []\n",
    "\n",
    "# Iterate through the list of keywords and use requests.get(...) to collect article-items \n",
    "# from each respective search results page.\n",
    "for keyword in keywords_list:\n",
    "    webpage = requests.get(\n",
    "        url = nytimes,\n",
    "        params = { \n",
    "        \"query\" : keyword ,\n",
    "        \"types\" : \"article\" ,\n",
    "        \"startDate\" : \"20200901\" ,\n",
    "        \"endDate\" : \"20200910\" ,\n",
    "        } \n",
    "    )\n",
    "    \n",
    "    print(\"Searching for keyword: {}\".format(keyword))\n",
    "    print(webpage.url)\n",
    "    \n",
    "    # Create the soup\n",
    "    soup = BeautifulSoup(markup = webpage.text)\n",
    "    \n",
    "    # Find and display the targetted results...\n",
    "    results = results + soup.find_all(class_=\"css-2fgx4k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the newly updated results object\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4 - Practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the outline below, conduct your own URL parameter-based search using the following prompts\n",
    "\n",
    "# keywords_list : define your own list of keywords to search\n",
    "\n",
    "# params\n",
    "# \"types\" : \"article\"\n",
    "# \"starDate\" : \"YYYYMMDD\" (choose your own start date)\n",
    "# \"endDate\" : \"YYYYMMDD\" (choose your own end date)\n",
    "\n",
    "# soup - Using find_all(...) identify the HTML class_ that identifies the byline of each search result (i.e. the descriptive sentences that are directly under the article-titles)\n",
    "\n",
    "\n",
    "\n",
    "# Import modules\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Specify a webpage\n",
    "nytimes = 'https://www.nytimes.com/search'\n",
    "\n",
    "# Define list of keywords\n",
    "keywords_list = [\"???\",\"???\",\"???\",\"???\"]\n",
    "\n",
    "# Create empty list of results to add items to\n",
    "results = []\n",
    "\n",
    "# Iterate through the list of keywords and use requests.get(...) to collect article-items \n",
    "# from each respective search results page.\n",
    "for keyword in keywords_list:\n",
    "    webpage = requests.get(\n",
    "        url = nytimes,\n",
    "        ??? = { \n",
    "        \"query\" : keyword ,\n",
    "        \"???\" : \"???\" ,\n",
    "        \"???\" : \"???\" ,\n",
    "        \"???\" : \"???\" ,\n",
    "        } \n",
    "    )\n",
    "    \n",
    "    print(\"Searching for keyword: {}\".format(keyword))\n",
    "    print(webpage.url)\n",
    "    \n",
    "    # Create the soup\n",
    "    soup = BeautifulSoup(markup = webpage.text)\n",
    "    \n",
    "    # Find and display the targetted results...\n",
    "    results = results + soup.find_all(class_=\"css-2fgx4k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the results\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 5 - Selenium WebDriver\n",
    "Not all websites contain \"static\" content.  In many cases, websites will dynamically generate content using Javascript and a variety of adjacent tools.  If you think about websites like Facebook or LinkedIn that allow you to scroll \"infinitely\" down, those are examples of dynamic content.  This type of content CAN NOT be captured by simply using the default Requests or BeautifulSoup packages.  We have to use an automated-browser of some form in order to scrape dynamic content.\n",
    "\n",
    "Note: this section introduces multiple complex addition to web-scraping processes.  It may be challenging to get everything to work on your computer.  It may be challenging to understand the tools and methods overall.  It may be challenging.  Stick with it though and you'll get there!\n",
    "\n",
    "Core Python Selenium Module:\n",
    "- https://selenium-python.readthedocs.io/\n",
    "- __(Installation Guide)__ https://selenium-python.readthedocs.io/installation.html#installation\n",
    "\n",
    "Two Unofficial Guides (many more exist though)\n",
    "- https://towardsdatascience.com/web-scraping-using-selenium-python-8a60f4cf40ab\n",
    "- https://www.scrapingbee.com/blog/selenium-python/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Selenium\n",
    "You may receive an error if you do not have the Python Selenium module installed (it is not included with Anaconda by default).  To install it, you will need to launch the \"Anaconda Prompt\" on your computer and type the following into the command line:\n",
    "\n",
    "_conda install selenium_\n",
    "\n",
    "For further info on this, please see the documentation: https://docs.anaconda.com/anaconda/user-guide/tasks/install-packages/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import BeautifulSoup4\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# From the selenium package, we need to import the webdriver and Keys functions.\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create \"driver\" object and start driving the Chromedriver using Python commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a driver object using webdriver.Chrome(...).  This is the object that\n",
    "# will be used to steer and command a standalone version of Chrome.\n",
    "\n",
    "# This will launch an independent Chrome browser\n",
    "driver = webdriver.Chrome( executable_path=\"chromedriver.exe\")\n",
    "driver.implicitly_wait(5)\n",
    "driver.set_window_size(1200, 1000)  # forces the Chrome window to a specific resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tell Chrome to go to a specific webpage\n",
    "driver.get(url=\"https://www.nytimes.com\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect current URL\n",
    "driver.current_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Begin isolating and targetting elements of the webpage.\n",
    "# In this case, find the \"Search\" icon\n",
    "search_button = driver.find_element_by_class_name(\"css-fnhm75\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Once identified, driver can \"click\" on the targetted element on screen\n",
    "search_button.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify and select the text-field for the search box\n",
    "search_field = driver.find_element_by_class_name(\"css-1axrnfw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use .send_keys(...) to type letters into the search_field\n",
    "search_field.send_keys(\"covid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Once text is entered in the search_field, send an \"ENTER\" command\n",
    "search_field.send_keys(Keys.ENTER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect current URL\n",
    "driver.current_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create soup and extract results\n",
    "soup = BeautifulSoup( markup = driver.page_source )\n",
    "results = soup.find_all(class_=\"css-2fgx4k\")\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close the browser\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 5 - ONE GIANT EXAMPLE\n",
    "The following block of code represents a sequence that uses Selenium WebDriver to systematically collect the first 30-40 results from the State Department's Press Releases webpages.  It's a lot to take in all at once, but nearly all of the key functional components are drawn from other material covered in this workshop series.  Take your time when trying to read through it and understand it.\n",
    "\n",
    "Note: This does not necessarily represent the most efficient way to write and execute the code for the task at hand.  It is structured so as to be as explanatory and as an instructional as possible.  As your skills develop, you will likely find many ways to make this code more succinct and efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import BeautifulSoup4\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Import Selenium webdriver\n",
    "from selenium import webdriver\n",
    "\n",
    "# Import a few utilities\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# Create an empty dataframe to store results in\n",
    "data = pd.DataFrame(columns=[\"date\",\"title\",\"url\",\"content\"])\n",
    "\n",
    "# Define starting base_url\n",
    "base_url = \"https://www.state.gov/press-releases/\"\n",
    "\n",
    "# Initiate a webdriver using chromedriver\n",
    "driver = webdriver.Chrome( executable_path=\"chromedriver.exe\")\n",
    "driver.implicitly_wait(1)\n",
    "driver.set_window_size(1000, 1200)\n",
    "\n",
    "# Go to starting press release directory page ()\n",
    "driver.get( url = base_url )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display press release directory URL\n",
    "print(\"URL Directory Page: {}\".format(driver.current_url))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Use Beautiful soup to find all of the links to individual press releases\n",
    "# and store the links in a \"results\" object.\n",
    "soup = BeautifulSoup(markup=driver.page_source)\n",
    "collection_results = soup.find_all(class_=\"collection-result\")\n",
    "collection_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for item in collection_results:\n",
    "    link = item.find(\"a\")[\"href\"]\n",
    "    results.append(link)\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Go to the first result\n",
    "driver.get(results[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through the results and use the driver to go to each link and harvest data\n",
    "for i in results:\n",
    "    # go to press release document\n",
    "    driver.get(i)  \n",
    "\n",
    "    # create soup from the document page\n",
    "    soup = BeautifulSoup(markup=driver.page_source) \n",
    "\n",
    "    # create objects for each piece of information to be stored in the data object\n",
    "    url = i\n",
    "    title = soup.find(class_=\"featured-content__headline\").text\n",
    "    date = soup.find(class_=\"article-meta__publish-date\").text\n",
    "    content = soup.find(class_=\"entry-content\").text\n",
    "\n",
    "    # Append harvested information to the data object\n",
    "    data = data.append({\"date\":date,\"title\":title,\"url\":url,\"content\":content}, ignore_index=True)\n",
    "\n",
    "    # display the trimmed text of the document-title\n",
    "    print(\"Doc Title: {}\".format( title.strip()) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send the driver back to the base_url, representing the press-release directory\n",
    "driver.get( url = base_url )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the data\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do a little data cleanup\n",
    "data[\"title\"] = data[\"title\"].str.strip()\n",
    "data[\"content\"] = data[\"content\"].str.strip()\n",
    "data[\"date\"] = pd.to_datetime(data[\"date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the data\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check to see if there is a \"next button\" in the directory\n",
    "\n",
    "next_button = driver.find_element_by_css_selector(\"a.next.page-numbers\")\n",
    "next_page = next_button.get_attribute(\"href\")\n",
    "base_url = next_page\n",
    "base_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send the driver to the newly updated base_url\n",
    "driver.get( url = base_url )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close the driver\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 5 - ONE GIANT EXAMPLE (ALTOGETHER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import BeautifulSoup4\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Import Selenium webdriver\n",
    "from selenium import webdriver\n",
    "\n",
    "# Import a few utilities\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# Create an empty dataframe to store results in\n",
    "data = pd.DataFrame(columns=[\"date\",\"title\",\"url\",\"content\"])\n",
    "\n",
    "# Define starting base_url\n",
    "base_url = \"https://www.state.gov/press-releases/\"\n",
    "\n",
    "# Initiate a webdriver using chromedriver\n",
    "driver = webdriver.Chrome( executable_path=\"chromedriver.exe\")\n",
    "driver.implicitly_wait(1)\n",
    "driver.set_window_size(1000, 1200)\n",
    "\n",
    "# Establish counter that will keep track of how many results have been collected\n",
    "counter = 0\n",
    "\n",
    "# Create a while loop (https://www.tutorialspoint.com/python/python_while_loop.htm)\n",
    "# So long as counter is less than or equal to 30, the process will repeat\n",
    "while counter < 30:\n",
    "    # Go to starting press release directory page ()\n",
    "    driver.get( url = base_url )\n",
    "    \n",
    "    # Display press release directory URL\n",
    "    print(\"URL Directory Page: {}\".format(driver.current_url))\n",
    "    print()\n",
    "    \n",
    "    # Use Beautiful soup to find all of the links to individual press releases\n",
    "    # and store the links in a \"results\" object.\n",
    "    soup = BeautifulSoup(markup=driver.page_source)\n",
    "    collection_results = soup.find_all(class_=\"collection-result\")\n",
    "    \n",
    "    results = []\n",
    "    for item in collection_results:\n",
    "        link = item.find(\"a\")[\"href\"]\n",
    "        results.append(link)\n",
    "    print(\"List of links to individual press release documents:\")\n",
    "    print(results)\n",
    "    print()\n",
    "    \n",
    "    # Iterate through the results and use the driver to go to each link and harvest data\n",
    "    for i in results:\n",
    "        # go to press release document\n",
    "        driver.get(i)  \n",
    "        \n",
    "        # create soup from the document page\n",
    "        soup = BeautifulSoup(markup=driver.page_source) \n",
    "        \n",
    "        # create objects for each piece of information to be stored in the data object\n",
    "        url = i\n",
    "        title = soup.find(class_=\"featured-content__headline\").text\n",
    "        date = soup.find(class_=\"article-meta__publish-date\").text\n",
    "        content = soup.find(class_=\"entry-content\").text\n",
    "        \n",
    "        # Append harvested information to the data object\n",
    "        data = data.append({\"date\":date,\"title\":title,\"url\":url,\"content\":content}, ignore_index=True)\n",
    "        \n",
    "        # display the trimmed text of the document-title\n",
    "        print(\"Doc Title: {}\".format( title.strip()) )\n",
    "    \n",
    "    # Update the counter to reflect the current number of rows in the data object\n",
    "    counter = data.shape[0]\n",
    "    print()\n",
    "    print(\"Total documents collected so far...{}\".format(counter))\n",
    "    print()\n",
    "    \n",
    "    # Send the driver back to the base_url, representing the press-release directory\n",
    "    driver.get( url = base_url )\n",
    "    \n",
    "    # Try to find a \"next button\" and extract the URL for the next page in the directory.\n",
    "    # If there is an error (\"except\"), then break and exit the while-loop.\n",
    "    try:\n",
    "        next_button = driver.find_element_by_css_selector(\"a.next.page-numbers\")\n",
    "        next_page = next_button.get_attribute(\"href\")\n",
    "        base_url = next_page\n",
    "    except:\n",
    "        break\n",
    "\n",
    "# Close the driver\n",
    "driver.close()\n",
    "\n",
    "# Do a little data cleanup\n",
    "data[\"title\"] = data[\"title\"].str.strip()\n",
    "data[\"content\"] = data[\"content\"].str.strip()\n",
    "data[\"date\"] = pd.to_datetime(data[\"date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect your data\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save your work!\n",
    "data.to_csv(\"state_dept.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 5 - Practice!\n",
    "This practice exercise is radically more simple than the example above. Overall, you have a few specific objectives:\n",
    "- First, make sure you can successfully import selenium webdrive, create a driver object to launch a browser, then go to the NYTimes search-page\n",
    "- Second, using Python/Selenium to type search terms into the search field\n",
    "- Third, change the \"Date Range\" filter to \"Yesterday\"\n",
    "- Fourth, use BeautifulSoup to create a soup of the search-results page\n",
    "- Fifth, identify and extract the headline tags for each article on screen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import BeautifulSoup4\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Import Selenium webdriver\n",
    "from selenium import webdriver\n",
    "\n",
    "# Define starting base_url\n",
    "base_url = \"https://www.nytimes.com/search\"\n",
    "\n",
    "# Initiate a webdriver using chromedriver\n",
    "driver = webdriver.Chrome( executable_path=\"chromedriver.exe\")\n",
    "driver.implicitly_wait(1)\n",
    "# driver.set_window_size(1000, 1200) # uncomment this line and edit if needed\n",
    "\n",
    "# Send the driver to the NYT search page\n",
    "driver.get(url = base_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Type text into the search field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the search text field by its class name\n",
    "search_field = driver.find_element_by_class_name(\"???\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send the keyword \"orange\" or something else silly to the search box\n",
    "search_field.send_keys(\"????\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send the following command to CTRL+A (i.e. \"select all\") text\n",
    "search_field.send_keys(Keys.CONTROL + \"a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the Keys.DELETE command to delete the selected text\n",
    "search_field.send_keys(Keys.???)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use .send_keys(...) again to send the keyword \"covid\"\n",
    "search_field.???(\"???\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use .send_keys(...) and Keys.ENTER to submit the search term\n",
    "search_field.???(Keys.???)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Select the \"Date Range\" menu button and click on \"Yesterday\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the \"Date Range\" element.\n",
    "\n",
    "# For the following practice exercises, there are many ways you may be able to \n",
    "# identify the element by class name, css selector, or xpath.\n",
    "\n",
    "# driver.find_element_by_class_name(...)\n",
    "# driver.find_element_by_css_selector(...)\n",
    "# driver.find_element_by_xpath(...)\n",
    "\n",
    "# Hint: For this button, there is a <div> tag that has a role=\"form\" as its parent.\n",
    "# You can use the class=\"...\" from that <div> to identify the button.\n",
    "date = driver.find_element_by_class_name(\"???\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the .click() method\n",
    "date.???()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the element for the \"Yesterday\" option in the \"Date Range\" menu\n",
    "yesterday = driver.???(\"???\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Click on \"Yesterday\"\n",
    "yesterday.???()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a soup and extract a list of article-titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a soup object using the driver.page_source for the HTML markup\n",
    "soup = BeautifulSoup(markup=???.???)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the soup, use .find_all(...) to identify the list of headline tags.\n",
    "# It is sufficient to simply capture the <li> or <h4> tags that contain\n",
    "# the headline text and info.\n",
    "\n",
    "titles = soup.find_all(name = \"???\", class_=\"???\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the headline titles\n",
    "titles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Close the driver!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, close the driver.\n",
    "driver.???()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
